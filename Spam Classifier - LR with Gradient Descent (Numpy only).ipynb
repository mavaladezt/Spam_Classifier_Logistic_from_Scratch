{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS and Email Spam Classifier using Logistic Regression and Gradient Descent (only numpy)\n",
    "#### (Applying Gradient Descent only with numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gracient Descent\n",
    "The gradient helps us maximize a function. So, gradient descent (negative gradient) helps us minimize a function when a global minimum exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was obtained from:<br>\n",
    "__Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). Irvine, CA: University of California, School of Information and Computer Science.__<br>\n",
    "\n",
    "Abstract: The SMS Spam Collection is a public set of SMS labeled messages that have been collected for mobile phone spam research.<br>\n",
    "\n",
    "Examples:<br>\n",
    "__ham__ What you doing?how are you?<br>\n",
    "__ham__ Ok lar... Joking wif u oni...<br>\n",
    "__ham__ dun say so early hor... U c already then say...<br>\n",
    "__ham__ MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*<br>\n",
    "__ham__ Siva is in hostel aha:-.<br>\n",
    "__ham__ Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.<br>\n",
    "__spam__ FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop<br>\n",
    "__spam__ Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B<br>\n",
    "__spam__ URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "Loading data first into spam/no spam so I can decide later which features (words/characters/punctuation) to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file\n",
    "y_spam=list()\n",
    "y_no_spam=list()\n",
    "sms_spam=list()\n",
    "sms_no_spam=list()\n",
    "\n",
    "#will also change ham/spam labels to -1/1 to make the math faster when measuring accuracy\n",
    "with open('SMSSpamCollection', 'r') as file:\n",
    "    reader = csv.reader(file,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0]=='ham':\n",
    "            y_no_spam.append(-1)\n",
    "            sms_no_spam.append(row[1])\n",
    "        else:\n",
    "            y_spam.append(1)\n",
    "            sms_spam.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(text):\n",
    "    # This function will perform some changes to the string received. It will first separate\n",
    "    #   some characters so that the combination of total words can be reduced significantly.\n",
    "    #   Then it will remove the stopwords that are always present in sms/emails that doesn't\n",
    "    #   give relevant information.\n",
    "    #   List of stopwords was taken from: https://gist.github.com/sebleier/554280 but I removed\n",
    "    #   some words ('no', 'nor' and 'not').\n",
    "    # Input: \n",
    "    # text : string with complete sms or email message\n",
    "    # Output:\n",
    "    # modified_text: string separating characters/words in 'separators' list and removing stop words \n",
    "    separators=['-','$','Â£',':','+','?','!','/','>','<','*','@','http','.com','www','(',')',\"'\",'.']\n",
    "    stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    text=text.lower()\n",
    "    for separator in separators:\n",
    "        text=text.replace(separator, ' ' + separator + ' ')\n",
    "    text=text.replace(\"   \", \" \")\n",
    "    text=text.replace(\"  \", \" \")\n",
    "    modified_text=str()\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            modified_text=modified_text + word + ' '\n",
    "    return modified_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Processing and Selection\n",
    "The process I will follow is really 'easy'.\n",
    " 1. Will get the top 5% of the words/characters with more 'hits' in the spam texts/emails.\n",
    " 2. Will do the same with the no-spam texts.\n",
    " 3. Will put both in a set (to remove duplicates) named 'features'.\n",
    "\n",
    "I decided to get only 5% of the top features (total words/characters were around 5k) initially because it had a great balance of enough data and speed.\n",
    "As you will see, I made different runs with more and less features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will generate 2 'big' string. One with all the spam texts concatenated and the other one with all the no-spam ones.\n",
    "#this texts are the ones that are going to help me determine the features.\n",
    "spam_text=standardize_data( \" \".join(sms_spam) )\n",
    "no_spam_text=standardize_data( \" \".join(sms_no_spam) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_decision(spam_text,no_spam_text,top):\n",
    "    # This function receive 2 texts and will select the top features in each.\n",
    "    # Input:\n",
    "    # spam_text : string\n",
    "    # no_spam_text : string\n",
    "    # top: proportion of top words/characters to include. Example: 0.05 will be the top 5% of the features\n",
    "    # Output:\n",
    "    # features: union of the 2 set of words/characters from the spam and no-spam texts\n",
    "\n",
    "    total = Counter()\n",
    "    spam = Counter()\n",
    "    no_spam = Counter()\n",
    "\n",
    "    for word in spam_text.split():\n",
    "        spam[word] += 1\n",
    "        total[word] += 1\n",
    "    for word in no_spam_text.split():\n",
    "        no_spam[word] += 1\n",
    "        total[word] += 1\n",
    "\n",
    "    spam_most_common=dict(spam.most_common()[0:int(len(spam)*top)])\n",
    "    no_spam_most_common=dict(no_spam.most_common()[0:int(len(no_spam)*top)])\n",
    "\n",
    "    top_spam_features=set()\n",
    "    top_no_spam_features=set()\n",
    "\n",
    "    for k,v in spam_most_common.items():\n",
    "        top_spam_features.add(k)\n",
    "\n",
    "    for k,v in no_spam_most_common.items():\n",
    "        top_no_spam_features.add(k)\n",
    "\n",
    "    return (top_spam_features | top_no_spam_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of items in the set Features:\n",
      "\n",
      "['time', 'draw', 'saw', 'dont', 'watching', 'well', 'dinner', 'stuff', 'new', 'week', 'help', 'princess']\n",
      "['6', 'start', 'makes', 'chance', 'office', 'still', 'asked', 'line', 'msg', 'always', 'valid', 'til']\n"
     ]
    }
   ],
   "source": [
    "#will run the previous function to select some features\n",
    "features=feature_decision(spam_text,no_spam_text,0.05)\n",
    "print('Example of items in the set Features:\\n')\n",
    "print(list(features)[0:12])\n",
    "print(list(features)[-12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates the vectors according to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(text,features):\n",
    "    # Function receives a string and returns the 'vector' with the features count in the text. It will count each\n",
    "    #   feature word in the text\n",
    "    # Input:\n",
    "    # text: string\n",
    "    # features: features set with top words\n",
    "    # Output:\n",
    "    # array: 1 array with n dimensions. Dimensions are equivalent to the size of features set.\n",
    "    \n",
    "    text=standardize_data(text)\n",
    "    dict_features = { i : 0 for i in features }\n",
    "    for i in text.split():\n",
    "        try:\n",
    "            dict_features[i]+=1\n",
    "        except:\n",
    "            continue\n",
    "    return np.array(list(dict_features.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The following process takes a little bit of time. In my computer takes around 15 seconds (Mac 2019).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.695\n",
      "\n",
      "x_train size:  (3875, 475) \ty_train size:  (3875,)\n",
      "\n",
      "x_test size:  (1697, 475) \ty_test size:  (1697,)\n"
     ]
    }
   ],
   "source": [
    "def create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features):\n",
    "    # Function randomly creates training and testing datasets.\n",
    "    #   1: it creates the vectors. 2: concatenates the y's values.  3: adds a column with random values\n",
    "    #   4: uses the random values to divide the train from the test arrays.\n",
    "    # Input:\n",
    "    # y_spam: list of 1's\n",
    "    # sms_spam: list with sms/emails spam messages\n",
    "    # y_no_spam: list of -1's\n",
    "    # sms_no_spam: list with sms/emails no spam messages\n",
    "    # features: features set (words/characters we will be looking for)\n",
    "    # Output:\n",
    "    # x_train: columns same size as the size of the features. Rows depend, but approximately 70% of total.\n",
    "    # y_train: 1 column. Same amount of rows than x_train. Values -1 or 1.\n",
    "    # x_test: columns same size as the size of the features. Rows depend, but approximately 70% of total.\n",
    "    # y_test: 1 column. Same amount of rows than x_test. Values -1 or 1.\n",
    "    \n",
    "    #1: creates vector matrix\n",
    "    for i in range (len(y_spam)):\n",
    "        feature_vector=create_vector(sms_spam[i],features)\n",
    "        feature_vector=feature_vector.reshape(1,feature_vector.shape[0])\n",
    "        if i == 0:\n",
    "            X_spam=feature_vector\n",
    "        else:\n",
    "            X_spam=np.concatenate([X_spam, feature_vector],axis=0)\n",
    "\n",
    "    for i in range (len(y_no_spam)):\n",
    "        feature_vector=create_vector(sms_no_spam[i],features)\n",
    "        feature_vector=feature_vector.reshape(1,feature_vector.shape[0])\n",
    "        if i == 0:\n",
    "            X_no_spam=feature_vector\n",
    "        else:\n",
    "            X_no_spam=np.concatenate([X_no_spam, feature_vector],axis=0)\n",
    "\n",
    "    X=np.concatenate([X_spam, X_no_spam],axis=0)\n",
    "\n",
    "    #2: concatenates y's\n",
    "    y=np.concatenate([y_spam, y_no_spam],axis=0)\n",
    "    X_and_y=np.concatenate([X,y.reshape(y.shape[0],1)],axis=1)\n",
    "\n",
    "    #3: random numbers column\n",
    "    random_column = np.array(np.random.rand(len(y)))  #create random column so I can divide train/test sets\n",
    "    random_column = random_column.reshape(random_column.shape[0],1)\n",
    "    X_and_y=np.concatenate([X_and_y,random_column],axis=1)\n",
    "    train=X_and_y[X_and_y[:,-1]<=.7]\n",
    "    test=X_and_y[X_and_y[:,-1]>.7]\n",
    "\n",
    "    print(\"Proportion of data used for training:\",round(train.shape[0]/(train.shape[0]+test.shape[0]),3 ))\n",
    "\n",
    "    #Train features and y\n",
    "    train=train[:,0:train.shape[1]-1]          #eliminate the random column, we don't need it anymore\n",
    "    y_train=train[:,-1]                        #get y from dataset (which is now the lat column)\n",
    "    x_train=train[:,0:train.shape[1]-1]        #eliminating the y column, this is now our feature set\n",
    "\n",
    "    test=test[:,0:test.shape[1]-1]\n",
    "    y_test=test[:,-1]\n",
    "    x_test=test[:,0:test.shape[1]-1]\n",
    "            \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "print('\\nx_train size: ',x_train.shape, '\\ty_train size: ',y_train.shape)\n",
    "print('\\nx_test size: ',x_test.shape, '\\ty_test size: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Functions and Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Implements sigmoid function. Base \n",
    "    # Input: \n",
    "    # z : scalar or array of any dimension\n",
    "    # Output:\n",
    "    # sgmd: scalar if received a scalar, array if receives an array (same dimensions)\n",
    "    sgmd = 1 / (1 + np.exp(-z))    \n",
    "    return sgmd\n",
    "\n",
    "def y_pred(X, w, b=0):\n",
    "    # Applies dot product of X and the weight vector. It then adds the bias.\n",
    "    # This information is used to predict the \"probability\" of the vector being label spam (1).\n",
    "    # Input:\n",
    "    # X: matrix (any size)\n",
    "    # w: vector with same dimensions as in X\n",
    "    # b: optional number, if not passed on is treated as 0.\n",
    "    # Output:\n",
    "    # prob: n-dimensional vector\n",
    "    prob=sigmoid(np.dot(X, w)+b)    \n",
    "    return prob\n",
    "\n",
    "def log_loss(X, y, w, b=0):\n",
    "    # Calculates the \"probability\" (likelihood) of having a label of 1\n",
    "    # Input:\n",
    "    # X: matrix\n",
    "    # y: vector with labels (+1 or -1)\n",
    "    # w: 1 row vector with same dimensins as X\n",
    "    # Output:\n",
    "    # a scalar\n",
    "    nll=-np.sum(np.log(sigmoid(y*(np.dot(X, w)+b))))\n",
    "    return nll\n",
    "\n",
    "def gradient(X, y, w, b):\n",
    "    # calculates the gradient\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # y: n-dimensional vector with labels (+1 or -1)\n",
    "    # w: d-dimensional vector\n",
    "    # b: a scalar bias term\n",
    "    # Output:\n",
    "    # wgrad: d-dimensional vector with gradient\n",
    "    # bgrad: a scalar with gradient\n",
    "    n, d = X.shape\n",
    "    weight_gradient = np.zeros(d)\n",
    "    bias_gradient = 0.0\n",
    "    bias_gradient = np.sum(-y*sigmoid(  -y*(np.dot(X, w)+b)  ) )\n",
    "    weight_gradient = np.dot((-y*sigmoid(  -y*(np.dot(X, w)+b)  ) ), X)\n",
    "    return weight_gradient, bias_gradient\n",
    "\n",
    "def logistic_regression(X, y, iterations, alpha):\n",
    "    # Run the logistic regression in a fixed amount of iterations. During each iteration it calculates the gradient,\n",
    "    #   applies it to the weight vector and bias number and calculates the loss.\n",
    "    # Input:\n",
    "    # X: matrix\n",
    "    # y: n-dimensional vector with labels (+1 or -1)\n",
    "    # iterations: number of iterations \n",
    "    # alpha: learning rate (very small number will take too long to find global minimum, too big will\n",
    "    #   never find it because it will be \"bouncing back\")\n",
    "    # Output:\n",
    "    # w: final weight vector\n",
    "    # b: bias number\n",
    "    # losses: list with calculated log-likelihood\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "    losses = np.zeros(iterations)\n",
    "    \n",
    "    for step in range(iterations):\n",
    "        wstep,bstep=gradient(X, y, w, b)\n",
    "        w-=alpha*wstep\n",
    "        b-=alpha*bstep\n",
    "        losses[step]=log_loss(X, y, w, b)\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Logistic Regression with Gradient Descent\n",
    "Tried different combinations of variables (size of alpha, number of iterations and amount of features).\n",
    "Also tried to have a variable alpha that was decreasing with iterations with no 'better' much success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features=feature_decision(spam_text,no_spam_text,0.05)  #already loaded before, don't need to run it again\n",
    "weight, b, losses = logistic_regression(x_train, y_train, 2000, 0.0008)  #5% top features/iterations 2000/alpha 0.0008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss value')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGDCAYAAAAxsvoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deZxcV33n/c+vqru1y5Js2cjyDgJjGGJAMSaEPASItwdiyEJMEvCQZJzkgUmYQCaG5AkMCc+QycIMzxAyZjCYBHCcEIICToxDWEIIxrJjjI0xFl6wLC+yJVu71Mtv/rinpOp2d6urUKluSZ/361WvunXqLud0dau+OufceyMzkSRJGkSNfldAkiSpWwYZSZI0sAwykiRpYBlkJEnSwDLISJKkgWWQkSRJA8sgI2mgRcSLI+LOPtfh7RHxv/tZB+loFV5HRjr6RMS9wC9l5j/2uy6HWq/bFhEvAf4iM0/qxf4ldcYeGUkqouK/i9IA8Q9W0iQR8R8iYkNEbImIdRFxYimPiHhvRDwSEU9ExK0R8ezy3kUR8a2I2B4RD0TEW6fZ77yIeLy1TSlbGRG7I+L4iDguIj5T1tkSEf88l1ARES+JiI1l+c+BU4C/i4gdEfGfS/m5EfHVsu9vlF6V1vZfjIh3R8S/ALuAMyLiDRFxR2nP3RHxy2XdRcDfAyeW/e+IiBMj4p0R8Rdt+/zxiLi9HO+LEfHMtvfujYi3lp/fExHxlxExv6MPSdJ+BhlJ+0XES4H/CrwGWAXcB1xd3j4P+BHg6cAy4GeAx8p7HwJ+OTOXAM8G/mnqvjNzL/A3wGvbil8DfCkzHwHeAmwEVgInAG8HOhr7zszXAd8DXpmZizPzv0XEauCzwO8DK4C3Ap+MiJVtm74OuAxYUtr8CPAKYCnwBuC9EfG8zNwJXAhsKvtfnJmb2usQEU8HPgG8ubTlWqpgNTKl3RcApwPPAf59J+2UdIBBRlK7nwOuzMybS/B4G/DCiDgNGKX6oj+Tan7dHZn5YNluFDgrIpZm5tbMvHmG/X+cyUHmZ0tZax+rgFMzczQz/zkPzSS+nweuzcxrM3MiM68H1gMXta3zkcy8PTPHyrE/m5nfzcqXgM8BL57j8X4G+GxmXp+Zo8AfAQuAH2pb532ZuSkztwB/B5z9/TZSOloZZCS1O5GqRwKAzNxB1euyOjP/CfifwPuBhyPiiohYWlb9SapgcF9EfCkiXjjD/v8JWBARL4iIU6m+wD9V3vtDYAPwuTKcc/khatOpwE+XYZ7HI+Jx4IepQlPL/e0bRMSFEfG1MsT1eGnbcXM83tSf4UTZ/+q2dR5qW94FLJ5zayRNYpCR1G4T1Rc/sH9OyLHAAwCZ+b7MfD7wLKohpt8s5Tdm5sXA8cDfAtdMt/PypX4NVa/MzwKfyczt5b3tmfmWzDwDeCXwGxHxsi7aMLUX537gzzNzWdtjUWa+Z7ptImIe8EmqnpQTMnMZ1fBQzLD/qab+DAM4mfIzlHRoGWSko9dwRMxvewxRDfO8ISLOLl/o/x9wQ2beGxE/WHpShoGdwB5gPCJGIuLnIuKYMpSyDRif5bgfpxp++TkODCsREa+IiKeVL/7WPmbbz0weBs5oe/0XwCsj4vyIaJa2viQiZjp9egSYB2wGxiLiQqr5Qe37PzYijplh+2uA/zsiXlZ+Vm8B9gJf7aItkg7CICMdva4Fdrc93pmZnwf+X6oeiQeBpwKXlPWXAh8EtlINnTxG1WsB1WTZeyNiG/ArVPNSppWZN1AFoROpzgBqWQP8I7AD+FfgTzPziwAR8fcR8fY5tuu/Ar9ThpHempn3AxdTTR7eTNVD85vM8O9f6SH6NapAspWq52hd2/vfpprMe3c5xolTtr+ztP//Bx6l6l16ZWbum2P9JXXAC+JJkqSBZY+MJEkaWAYZSZI0sHoWZMqEuq+Xq2jeHhH/pZR/JCLuiYhbyuPsUh4R8b6orih6a0Q8r21fl0bEXeVxaa/qLEmSBstQD/e9F3hpZu4oM/e/EhGtiX2/mZl/PWX9C6km+60BXgB8AHhBRKwA3gGspTrt8aaIWJeZW3tYd0mSNAB61iNTroi5o7wcLo/ZZhZfDHy0bPc1YFlErALOB67PzC0lvFxPdWlvSZJ0lOtljwwR0QRuAp4GvD8zb4iIXwXeHRG/C3weuLxcCn01k6+uubGUzVQ+9ViXUd0rhUWLFj3/zDPP7EGLJElSP9x0002PZubKqeU9DTKZOQ6cHRHLgE9Fddfbt1FdnnsEuAL4LeBdHLhq5qRdzFI+9VhXlP2xdu3aXL9+/SFpgyRJ6r+IuG+68sNy1lJmPg58EbggMx8sw0d7gQ8D55TVNlJdxrvlJKpLfc9ULkmSjnK9PGtpZemJISIWAC8Hvl3mvbTuP/Iq4LayyTrg9eXspXOBJ8qdda8DzouI5RGxnOpS4df1qt6SJGlw9HJoaRVwVZkn0wCuyczPRMQ/RcRKqiGjW6guZw7V5dIvorr77S7gDQCZuSUifg+4saz3rszc0sN6S5KkAXFE3qLAOTKSJB1ZIuKmzFw7tdwr+0qSpIFlkJEkSQPLICNJkgaWQUaSJA0sg4wkSRpYBhlJkjSwDDKSJGlgGWQ6cN9jO/nCnY8wMXHkXXtHkqRBZJDpwGdufZA3fPhGRicm+l0VSZKEQaYrR+DFkCVJGkgGmQ5E9LsGkiSpnUGmA4FJRpKkOjHIdMGhJUmS6sEg0wGHliRJqheDTBcSu2QkSaoDg0wHWh0yDi1JklQPBpkOOLQkSVK9GGS6YIeMJEn1YJDpQOv063RsSZKkWjDIdMChJUmS6sUg0wX7YyRJqgeDjCRJGlgGmS44RUaSpHowyHQgWpNkDDKSJNWCQaYDzvWVJKleDDJd8BYFkiTVg0GmA/tHlswxkiTVgkGmAw4tSZJULwaZLtghI0lSPRhkOhBe2leSpFoxyHTBey1JklQPBpkOeBkZSZLqxSDTAQeWJEmqF4NMFxxZkiSpHgwynShjS14QT5KkejDIdMChJUmS6qVnQSYi5kfE1yPiGxFxe0T8l1J+ekTcEBF3RcRfRsRIKZ9XXm8o75/Wtq+3lfI7I+L8XtV5zuyQkSSpFnrZI7MXeGlm/gBwNnBBRJwL/AHw3sxcA2wFfrGs/4vA1sx8GvDesh4RcRZwCfAs4ALgTyOi2cN6z8jLyEiSVC89CzJZ2VFeDpdHAi8F/rqUXwW8qixfXF5T3n9ZVFeguxi4OjP3ZuY9wAbgnF7Vey7skJEkqR56OkcmIpoRcQvwCHA98F3g8cwcK6tsBFaX5dXA/QDl/SeAY9vLp9nmsIoyS8azliRJqoeeBpnMHM/Ms4GTqHpRnjndauV5uoGbnKV8koi4LCLWR8T6zZs3d1vlWTm0JElSvRyWs5Yy83Hgi8C5wLKIGCpvnQRsKssbgZMByvvHAFvay6fZpv0YV2Tm2sxcu3Llyl4048CxHFySJKkWennW0sqIWFaWFwAvB+4AvgD8VFntUuDTZXldeU15/5+yuqnROuCSclbT6cAa4Ou9qvdsWh0yDi1JklQPQwdfpWurgKvKGUYN4JrM/ExEfAu4OiJ+H/g34ENl/Q8Bfx4RG6h6Yi4ByMzbI+Ia4FvAGPDGzBzvYb1n5NCSJEn10rMgk5m3As+dpvxupjnrKDP3AD89w77eDbz7UNexW3bISJJUD17ZtwPhtX0lSaoVg0wX0kkykiTVgkGmE6VDxhwjSVI9GGQ64MCSJEn1YpCRJEkDyyDTgQhvUSBJUp0YZDrg0JIkSfVikOmCtyiQJKkeDDId8Mq+kiTVi0GmC86RkSSpHgwyHWj1yJhjJEmqB4NMB7xFgSRJ9WKQ6YK3KJAkqR4MMh1waEmSpHoxyEiSpIFlkOmCI0uSJNWDQaYD4YVkJEmqFYNMV+ySkSSpDgwyHWj1xzi0JElSPRhkOuDIkiRJ9WKQ6YIdMpIk1YNBpgOtK/s6tCRJUj0YZDrg0JIkSfVikOlCOrgkSVItGGQ6YIeMJEn1YpDpgnNkJEmqB4NMB/bfNNIgI0lSLRhkOuLgkiRJdWKQ6YKTfSVJqgeDTAc8/VqSpHoxyHTBOTKSJNWDQaYDdshIklQvBpkOhGNLkiTVikGmCw4tSZJUDwaZDrT6YzxrSZKkejDIdMCRJUmS6sUg0wWHliRJqoeeBZmIODkivhARd0TE7RHx66X8nRHxQETcUh4XtW3ztojYEBF3RsT5beUXlLINEXF5r+p8MPbISJJUL0M93PcY8JbMvDkilgA3RcT15b33ZuYfta8cEWcBlwDPAk4E/jEinl7efj/wY8BG4MaIWJeZ3+ph3Wdlh4wkSfXQsyCTmQ8CD5bl7RFxB7B6lk0uBq7OzL3APRGxATinvLchM+8GiIiry7qHPchEme6bji1JklQLh2WOTEScBjwXuKEUvSkibo2IKyNieSlbDdzfttnGUjZT+dRjXBYR6yNi/ebNmw9xC1oH6c1uJUlSd3oeZCJiMfBJ4M2ZuQ34APBU4GyqHps/bq06zeY5S/nkgswrMnNtZq5duXLlIan7TOyPkSSpHno5R4aIGKYKMR/LzL8ByMyH297/IPCZ8nIjcHLb5icBm8ryTOWH1f7ryJhkJEmqhV6etRTAh4A7MvNP2spXta32auC2srwOuCQi5kXE6cAa4OvAjcCaiDg9IkaoJgSv61W9Z+MtCiRJqpde9si8CHgd8M2IuKWUvR14bUScTTVCcy/wywCZeXtEXEM1iXcMeGNmjgNExJuA64AmcGVm3t7Des+BXTKSJNVBL89a+grTz2+5dpZt3g28e5rya2fb7nCxP0aSpHrxyr5dcI6MJEn1YJDpQGuKjDlGkqR6MMh0IBxckiSpVgwyXXBoSZKkejDIdGD/0JJJRpKkWjDIdMCBJUmS6sUg0wX7YyRJqgeDTCfskpEkqVYMMl1wiowkSfVgkOlA6/TrdHBJkqRaMMh0wHtGSpJULwaZbtghI0lSLRhkOtDqkDHHSJJUDwaZDoRjS5Ik1YpBpguetSRJUj0YZDpgh4wkSfVikOmCp19LklQPBpkO7J/sa46RJKkWDDIdcGhJkqR6Mch0wQ4ZSZLqwSDTkXKLAseWJEmqBYNMBxxakiSpXgwyXbA/RpKkejDIdMAOGUmS6sUg0w27ZCRJqgWDTAda91rygniSJNWDQaYDDi1JklQvBpkuePa1JEn1YJDpQOv0a4OMJEn1YJDpQDi4JElSrRhkumCHjCRJ9WCQ6YBX9pUkqV4MMl3wXkuSJNWDQaYLxhhJkurBINMBh5YkSaoXg0wXHFmSJKkeDDIdOHD6tUlGkqQ66FmQiYiTI+ILEXFHRNweEb9eyldExPURcVd5Xl7KIyLeFxEbIuLWiHhe274uLevfFRGX9qrOB29Tv44sSZKm08semTHgLZn5TOBc4I0RcRZwOfD5zFwDfL68BrgQWFMelwEfgCr4AO8AXgCcA7yjFX76xaElSZLqoWdBJjMfzMyby/J24A5gNXAxcFVZ7SrgVWX5YuCjWfkasCwiVgHnA9dn5pbM3ApcD1zQq3rPxh4ZSZLq5bDMkYmI04DnAjcAJ2Tmg1CFHeD4stpq4P62zTaWspnKpx7jsohYHxHrN2/efKibMIkdMpIk1UPPg0xELAY+Cbw5M7fNtuo0ZTlL+eSCzCsyc21mrl25cmV3lT2I1mRfh5YkSaqHgwaZiHh6RHw+Im4rr58TEb8zl51HxDBViPlYZv5NKX64DBlRnh8p5RuBk9s2PwnYNEv5YefQkiRJ9TKXHpkPAm8DRgEy81bgkoNtFBEBfAi4IzP/pO2tdUDrzKNLgU+3lb++nL10LvBEGXq6DjgvIpaXSb7nlbK+SQeXJEmqhaE5rLMwM78ek7sjxuaw3YuA1wHfjIhbStnbgfcA10TELwLfA366vHctcBGwAdgFvAEgM7dExO8BN5b13pWZW+Zw/ENu/1VkzDGSJNXCXILMoxHxVMq8lIj4KeDBg22UmV9h+vktAC+bZv0E3jjDvq4ErpxDXXvKoSVJkuplLkHmjcAVwJkR8QBwD/DzPa1VzdkhI0lSPRw0yGTm3cDLI2IR0CjXhDlK2SUjSVKdHDTIRMTvTnkNQGa+q0d1qr10kowkSbUwl6GlnW3L84FXUF2l96jjHBlJkuplLkNLf9z+OiL+iOpU6aOOOUaSpHrp5sq+C4EzDnVFBokjS5Ik1cNc5sh8kwMn6jSBlcBROT9m//wgz1uSJKkW5jJH5hVty2PAw5k5lwviHXEcWpIkqV5mDDIRsaIsTj3demlE0K+r69aBQ0uSJNXDbD0yNzH73aePunkynrUkSVK9zBhkMvP0w1mRQWKPjCRJ9TCXOTKUu06vobqODACZ+eVeVaqugtZkX0mSVAdzOWvpl4BfB04CbgHOBf4VeGlvq1Y/Di1JklQvc7mOzK8DPwjcl5k/CjwX2NzTWtWctyiQJKke5hJk9mTmHoCImJeZ3wae0dtq1ZsxRpKkepjLHJmNEbEM+Fvg+ojYCmzqbbXqyaElSZLqZS73Wnp1WXxnRHwBOAb4h57Wqu7skpEkqRbmMtn3fwB/mZlfzcwvHYY61VbYJSNJUq3MZY7MzcDvRMSGiPjDiFjb60rVnfdakiSpHg4aZDLzqsy8CDgH+A7wBxFxV89rVkOt/hhPWpIkqR7m0iPT8jTgTOA04Ns9qU3NObIkSVK9HDTIRESrB+ZdwG3A8zPzlT2vWY3ZISNJUj3M5fTre4AXZuajva5M3e2/RYFJRpKkWpjL6dd/djgqMggcWpIkqV46mSOjwrOWJEmqB4NMB+yQkSSpXuYy2fepETGvLL8kIn6t3LLgqOUcGUmS6mEuPTKfBMYj4mnAh4DTgY/3tFZ1VbpkzDGSJNXDXILMRGaOAa8G/ntm/idgVW+rVU/h4JIkSbUylyAzGhGvBS4FPlPKhntXpQHg2JIkSbUwlyDzBuCFwLsz856IOB34i95Wq57CoSVJkmplLteR+RbwawARsRxYkpnv6XXF6siBJUmS6mUuZy19MSKWRsQK4BvAhyPiT3pftfpyZEmSpHqYy9DSMZm5DfgJ4MOZ+Xzg5b2tVj2Fl/aVJKlW5hJkhiJiFfAaDkz2PaqlXTKSJNXCXILMu4DrgO9m5o0RcQZwV2+rVU+t/hhjjCRJ9XDQIJOZf5WZz8nMXy2v787MnzzYdhFxZUQ8EhG3tZW9MyIeiIhbyuOitvfeFhEbIuLOiDi/rfyCUrYhIi7vvImHjiNLkiTVy1wm+54UEZ8qoeThiPhkRJw0h31/BLhgmvL3ZubZ5XFtOcZZwCXAs8o2fxoRzYhoAu8HLgTOAl5b1u0rR5YkSaqHuQwtfRhYB5wIrAb+rpTNKjO/DGyZYz0uBq7OzL2ZeQ+wATinPDaUXqB9wNVl3b5oXdnXHCNJUj3MJciszMwPZ+ZYeXwEWPl9HPNNEXFrGXpaXspWA/e3rbOxlM1U3h8OLUmSVCtzCTKPRsTPt4Z6IuLngce6PN4HgKcCZwMPAn9cyqeLCDlL+ZNExGURsT4i1m/evLnL6s2NZy1JklQPcwkyv0B16vVDVOHjp6huW9CxzHw4M8czcwL4INXQEVQ9LSe3rXoSsGmW8un2fUVmrs3MtStXfj8dRjNzsq8kSfUyl7OWvpeZP56ZKzPz+Mx8FdXF8TpWrkfT8mqgdUbTOuCSiJhX7uW0Bvg6cCOwJiJOj4gRqgnB67o5tiRJOvIc9F5LM/gN4L/PtkJEfAJ4CXBcRGwE3gG8JCLOphoeuhf4ZYDMvD0irgG+BYwBb8zM8bKfN1Fdx6YJXJmZt3dZ5+/b/uvIOLIkSVItdBtkDjrIkpmvnab4Q7Os/27g3dOUXwtc21HtesRbFEiSVC9zmSMznaO6TyKP7uZLklQbM/bIRMR2pg8sASzoWY1qzKElSZLqZcYgk5lLDmdFBoEjS5Ik1Uu3Q0tHNTtkJEmqB4NMB8JL+0qSVCsGmS44R0aSpHowyHSgNUfGs5YkSaoHg4wkSRpYBpkuOLQkSVI9GGQ64OnXkiTVi0GmA561JElSvRhkupCOLUmSVAsGmQ44tCRJUr0YZDrQyjETdshIklQLBpkONBtVlBk3yUiSVAsGmQ5EBBEw4RwZSZJqwSDToWaEQUaSpJowyHSoEcH4RL9rIUmSwCDTsUbDoSVJkurCINOhZoSTfSVJqgmDTIcaDYOMJEl1YZDpULPhZF9JkurCINMhz1qSJKk+DDIdCs9akiSpNgwyHWo2YMI5MpIk1YJBpkPNCMYdWpIkqRYMMh1qNMIeGUmSasIg06Fmwx4ZSZLqwiDToUYEdshIklQPBpkONcLJvpIk1YVBpkNNr+wrSVJtGGQ61PCsJUmSasMg06GmZy1JklQbBpkOedaSJEn1YZDp0HCzwaj3KJAkqRYMMh0aaTbYN2aQkSSpDgwyHRoZMshIklQXPQsyEXFlRDwSEbe1la2IiOsj4q7yvLyUR0S8LyI2RMStEfG8tm0uLevfFRGX9qq+czUy1GCvQUaSpFroZY/MR4ALppRdDnw+M9cAny+vAS4E1pTHZcAHoAo+wDuAFwDnAO9ohZ9+GWk22OccGUmSaqFnQSYzvwxsmVJ8MXBVWb4KeFVb+Uez8jVgWUSsAs4Hrs/MLZm5FbieJ4ejw8qhJUmS6uNwz5E5ITMfBCjPx5fy1cD9bettLGUzlT9JRFwWEesjYv3mzZsPecVbRjxrSZKk2qjLZN+YpixnKX9yYeYVmbk2M9euXLnykFaunT0ykiTVx+EOMg+XISPK8yOlfCNwctt6JwGbZinvG4OMJEn1cbiDzDqgdebRpcCn28pfX85eOhd4ogw9XQecFxHLyyTf80pZ34wMOdlXkqS6GOrVjiPiE8BLgOMiYiPV2UfvAa6JiF8Evgf8dFn9WuAiYAOwC3gDQGZuiYjfA24s670rM6dOID6sqjkyycRE0mhMN/IlSZIOl54Fmcx87QxvvWyadRN44wz7uRK48hBW7fsyMlR1Yu0bn2B+o9nn2kiSdHSry2TfgTGvLchIkqT+Msh0aH+PjBN+JUnqO4NMh4abBhlJkurCINOhEYOMJEm1YZDp0Lzh6kfmjSMlSeo/g0yHFs+rTvTasXe0zzWRJEkGmQ4tXTAMwLbdY32uiSRJMsh0aOn8EmT22CMjSVK/GWQ6tGR+NbS0fY89MpIk9ZtBpkMLRqqr+e4ZHe9zTSRJkkGmQwuGqyCze59BRpKkfjPIdGi42WCoEey2R0aSpL4zyHRhwXDTICNJUg0YZLqwYKTpHBlJkmrAINOFYxYMs2Xnvn5XQ5Kko55Bpgurly/ggcd397sakiQd9QwyXVixaIStO70gniRJ/WaQ6cKikSF27fOCeJIk9ZtBpgsLR5rs8joykiT1nUGmCwtGmuwdm2B8IvtdFUmSjmoGmS4sLLcpcHhJkqT+Msh0YdnCEQAe2+Ep2JIk9ZNBpgunrFgIwPe27OpzTSRJOroZZLpw3OJ5AGzdZY+MJEn9ZJDpwtIFQwBs2+McGUmS+skg04Wl84cB2L7Hi+JJktRPBpkuzBtqMNJs8MRug4wkSf1kkOlCRLBq2Xwe2Or9liRJ6ieDTJdOWbGQ+z1rSZKkvjLIdOmUFQu5zyAjSVJfGWS6dOqxC3l816jzZCRJ6iODTJdaF8VzeEmSpP4xyHTplBWLAK/uK0lSPxlkunTKsVWPzH2PGWQkSeoXg0yXFs8b4thFI/bISJLURwaZ78PJKxbyvS07+10NSZKOWn0JMhFxb0R8MyJuiYj1pWxFRFwfEXeV5+WlPCLifRGxISJujYjn9aPO0zn12IXc+6g9MpIk9Us/e2R+NDPPzsy15fXlwOczcw3w+fIa4EJgTXlcBnzgsNd0Bs9ctZQHHt/Nozv29rsqkiQdleo0tHQxcFVZvgp4VVv5R7PyNWBZRKzqRwWn+sHTVgCw/t6tfa6JJElHp34FmQQ+FxE3RcRlpeyEzHwQoDwfX8pXA/e3bbuxlPXdv1t9DPOGGtx475Z+V0WSpKPSUJ+O+6LM3BQRxwPXR8S3Z1k3pinLJ61UBaLLAE455ZRDU8uDGBlqcPbJy7jhnscOy/EkSdJkfemRycxN5fkR4FPAOcDDrSGj8vxIWX0jcHLb5icBm6bZ5xWZuTYz165cubKX1Z/k/3rGSm57YBsPPuGdsCVJOtwOe5CJiEURsaS1DJwH3AasAy4tq10KfLosrwNeX85eOhd4ojUEVQfnnfUUAD53+8N9rokkSUeffvTInAB8JSK+AXwd+Gxm/gPwHuDHIuIu4MfKa4BrgbuBDcAHgf/n8Fd5Zk87fjFPO34xn721NtlKkqSjxmGfI5OZdwM/ME35Y8DLpilP4I2HoWpd+6nnn8R7/v7bfOfh7Tz9hCX9ro4kSUeNOp1+PbBes/ZkRoYafOxr9/W7KpIkHVUMMofAikUjvOI5q7hm/UY2b/fieJIkHS4GmUPkTT/6NPaNT/D+L2zod1UkSTpqGGQOkTNWLuY1a0/mYzfcx50Pbe93dSRJOioYZA6h3zz/GSydP8xb/+objI5P9Ls6kiQd8Qwyh9CKRSP8/quezTcfeII/vO7OfldHkqQjnkHmELvw363ideeeyhVfvptr1t9/8A0kSVLX+nWvpSPa777yLO55dCeXf/JWRpoNXvXcWtzjUpKkI449Mj0w3Gxwxeufzzmnr+A3rrmFj/zLPVTX9ZMkSYeSQaZHFo4M8eF/fw4vPfN43vl33+K3Pnkru/eN97takiQdUQwyPbRgpMkVr1vLf3zp07hm/UYuet8/c+O9W/pdLUmSjhgGmR5rNIK3nPcMPv4fXsDo+ASv+V//ym9ccwubHt/d76pJkjTwDDKHyQ899Tiue/OPcNmLz+Aztz7Ij/7RF3nX332LBww0kiR1LY7ESahr167N9evX97saM7p/yy7e+4/f4dO3bCKAVzxnFT9/7qk8/9TlRES/qydJUu1ExE2ZufZJ5QaZ/tm4dRdXfuVerr7xe+zaN84Zxy3ip9aexI//wImctHxhv6snSVJtGGRqbOfeMT77zQf5q/X3c+O9WwF41olLOf9ZT+G8Z53AM05YYk+NJOmoZhggMpkAAA+mSURBVJAZEPc9tpN/uO0hrrv9IW7+3uMArFwyjx966rG86KnH8UNPO9beGknSUccgM4Ae2baHL9z5CP+y4TG++t3HeHTHXgBWHTOf556yjLNPXsZzT1nOs088hgUjzT7XVpKk3jHIDLjM5DsP7+Cr332Uf/ve4/zb/Vu5f0t1xlOzEaw5fjFnPmUJz3jKUs58yhLOXLWEpyyd75CUJOmIYJA5Aj26Yy/fuP9xbrn/cW7ftI07H9o+6XTupfOHeMZTlnDasYs47bhFnH7corK8kIUj3mZLkjQ4ZgoyfpsNsOMWz+NlzzyBlz3zhP1lT+we5c6HtnPnQ9u446HtbHh4B1/8zmY237Rx0rZPWTqfU49dyEnLF7J62XxWL1/AicsWsHpZ9Tx/2KEqSVL9GWSOMMcsGOac01dwzukrJpXv2DvGvY/u5N7HdnLP5p3c89hO7ntsF1/97qM8vG0PE1M65o5bPMKJyxaw6pj5HL9kPiuXzOP4JfM4fuk8jl8yn+OXzOPYxfNoNhy6kiT1j0HmKLF43hDPXn0Mz159zJPeGx2f4KEn9vDA47vZ9PhuHti6m01P7OaBx/dw9+ad3HDPFh7fNfqk7RoBxy6uAs7KJfM4dtE8ViwaZvmiEVYsHKmeF42wfGH1fMyCYYOPJOmQMsiI4WaDk1cs5OQVM5/WvXdsnM3b9/LI9r08sm0vm3fsZfO2PdXr7XvZvH0vdz28gy0797F7dPq7fDcCli0cYfnCYVYsGmHZwhGWzh9m6YIhjlkwXJaHWTq/vC6PYxYMs2ik6cRlSdKTGGQ0J/OGmpy0fOGcrmGze984W3ftY8vOfQeed1bPW3btY+vOUbbs3Mf9W3axfc8Y23aPsn3v2Kz7bAQl5AyXkDPEopEhFs8bYlF5LJ7XbFueUta27siQtxiTpCOFQUaH3IKRJgtGqknDczU+kWzfM8q23WNs2zPKE7tH2bZ7lG2l7In9y6Ns21O9fmzHLnbsHWPn3jF27h1n3/jEnI410mywqAScxfOGWDjSZOHIEPOHmywcabJguFna0GRh2/KC8n613tC0680bathzJEmHkUFGtdBsBMsWVsNN3do3NsHOvWNVuNlXBZzte6qQs7987xg79h0IPzv2jrF73zi79o3x2M597N43xu7RcXbtG2fP6Dij451dniCCSYFn/nAVbuYNNdqWm8wfrp7nDTdmXGde+zr7X0+z/VCT4WYYoCQdlQwyOmKMDDUYGaomGR8qo+MT7B4dZ8++Kty0h5zW6z0lCO0enZgUhHaPjrN3bIK95XnPaBWc9o5OsGdsnL2jE+wdG2dPef39XNIpAuYNNRhuVmFnpNlguPXcbJSfTfV6ZKjBcDMYKQFo3pT1Ws/zhp5cNtKM8tws+2g7ZlmuHsFws8FQMxhuNGg4yVtSjxhkpFm0vpiXzh/u6XEyk7GJZE8r/JTgM13o2Ts2OSDtX3dsgn1jE+wbn2C0PO8bm2B0vFpndHyCXfvGeHz3BKNjuf/9fePVe/vK9mNTz8U/BBoBQ80Gw41geKjBUKMKO62gM7Q/+FTr7H/diP2fwVAzJm/XKm9E23ZPDlFDzap8pGzfvu+hZtBsVMvNRrQ9N2g2g+Epr1vvNyMMZ1JNGGSkGoiI/V/AS/pcl4mJnBxuxlvBZ5x9Y/mk4NMemFrPo+PJ6HgVikbHJxgbT0YnynN5f2zq++MTjE6U8vFkx9hY2/rVumPt+x2b2L/PXoSvg2kEVcBpBaDmlCDUFoyas4amtvLm9OVDzSnrTTlmIw5s12gLWs2YXNbcXx9oxORQ1iz7ae17//tt6zba9tE6Zmvbaj/sP45DnTpcDDKSJmk0gvmN5kBd3bnVozU2XgWtqSFpbGKCfWPVc3uIGi+Parl6r/11a52xmconvT9NeWv98anlE2XYcur+JxifyJnr0Vbnuotgf0gamjYwMamsOSUkHQhgPClETQpgbes2yjEjynJj8nIjWg/K+mU5Jm/faMSB+kdZbjx522Z5r7E/vLWHPg4cr9G23Pa6tV6Ufc3pWK22t7ZrzNCeKT+DI5lBRtLAO9CjBQsYnADWrcxpglQpm8gDYWdiUhn7y6euO/GkMhifmKies7xf1plofy51aG0z3bFb645NOg6T99N27LHx9m2ZdOx9YxP71516nMxS10wmSl2qR7WP9vUmshw7D/wsByAbfl+eHJomB7hGWwBstAWl9hAWbe/NtP4rnrOKX3rxGYe1bQYZSRowEWW46cjPbIdVK/BM5OQgVAWgA4En24LQk7aZOLDNRE4OWNkW+FrrtUJVlgA2PnW9mcLZQY91oPzJxzoQJPcHu4kkYX89WsfJKcfMKT+bnPTzqk46ONwMMpIkUYZnOLKHYY5EXuJUkiQNLIOMJEkaWAYZSZI0sAYmyETEBRFxZ0RsiIjL+10fSZLUfwMRZCKiCbwfuBA4C3htRJzV31pJkqR+G4ggA5wDbMjMuzNzH3A1cHGf6yRJkvpsUILMauD+ttcbS9l+EXFZRKyPiPWbN28+rJWTJEn9MShBZroT+yddhzEzr8jMtZm5duXKlYepWpIkqZ8GJchsBE5ue30SsKlPdZEkSTUxKEHmRmBNRJweESPAJcC6PtdJkiT12UDcoiAzxyLiTcB1QBO4MjNv73O1JElSnw1EkAHIzGuBa/tdD0mSVB+DMrQkSZL0JJGZB19rwETEZuC+Hu3+OODRHu27LmzjkeNoaKdtPDLYxiNDL9t4amY+6bTkIzLI9FJErM/Mtf2uRy/ZxiPH0dBO23hksI1Hhn600aElSZI0sAwykiRpYBlkOndFvytwGNjGI8fR0E7beGSwjUeGw95G58hIkqSBZY+MJEkaWAaZDkTEBRFxZ0RsiIjL+12fbkXEyRHxhYi4IyJuj4hfL+XvjIgHIuKW8riobZu3lXbfGRHn96/2cxcR90bEN0tb1peyFRFxfUTcVZ6Xl/KIiPeVNt4aEc/rb+0PLiKe0fZZ3RIR2yLizYP+OUbElRHxSETc1lbW8ecWEZeW9e+KiEv70ZaZzNDGP4yIb5d2fCoilpXy0yJid9vn+Wdt2zy//I5vKD+H6W6w2xcztLHj3806/7s7Qxv/sq1990bELaV8UD/Hmb4v6vM3mZk+5vCgujXCd4EzgBHgG8BZ/a5Xl21ZBTyvLC8BvgOcBbwTeOs0659V2jsPOL38HJr9bscc2nkvcNyUsv8GXF6WLwf+oCxfBPw91Z3WzwVu6Hf9O2xrE3gIOHXQP0fgR4DnAbd1+7kBK4C7y/Pysry83207SBvPA4bK8h+0tfG09vWm7OfrwAtL+/8euLDfbTtIGzv63az7v7vTtXHK+38M/O6Af44zfV/U5m/SHpm5OwfYkJl3Z+Y+4Grg4j7XqSuZ+WBm3lyWtwN3AKtn2eRi4OrM3JuZ9wAbqH4eg+hi4KqyfBXwqrbyj2bla8CyiFjVjwp26WXAdzNztgtBDsTnmJlfBrZMKe70czsfuD4zt2TmVuB64ILe135upmtjZn4uM8fKy68BJ822j9LOpZn5r1l9U3yUAz+Xvpvhc5zJTL+btf53d7Y2ll6V1wCfmG0fA/A5zvR9UZu/SYPM3K0G7m97vZHZv/wHQkScBjwXuKEUval0B17Z6ipkcNuewOci4qaIuKyUnZCZD0L1BwocX8oHtY0tlzD5H8wj6XOEzj+3QW4rwC9Q/a+25fSI+LeI+FJEvLiUraZqV8ugtLGT381B/hxfDDycmXe1lQ305zjl+6I2f5MGmbmbbsxyoE/5iojFwCeBN2fmNuADwFOBs4EHqbpFYXDb/qLMfB5wIfDGiPiRWdYd1DYSESPAjwN/VYqOtM9xNjO1aWDbGhG/DYwBHytFDwKnZOZzgd8APh4RSxnMNnb6uzmIbWx5LZP/czHQn+M03xczrjpNWU8/S4PM3G0ETm57fRKwqU91+b5FxDDVL+XHMvNvADLz4cwcz8wJ4IMcGHYYyLZn5qby/AjwKar2PNwaMirPj5TVB7KNxYXAzZn5MBx5n2PR6ec2kG0tEyBfAfxcGWagDLc8VpZvopoz8nSqNrYPP9W+jV38bg7q5zgE/ATwl62yQf4cp/u+oEZ/kwaZubsRWBMRp5f/AV8CrOtznbpSxm4/BNyRmX/SVt4+J+TVQGsm/jrgkoiYFxGnA2uoJqfVVkQsioglrWWqiZS3UbWlNVv+UuDTZXkd8Poy4/5c4IlWt+kAmPQ/vyPpc2zT6ed2HXBeRCwvwxfnlbLaiogLgN8Cfjwzd7WVr4yIZlk+g+pzu7u0c3tEnFv+pl/PgZ9LLXXxuzmo/+6+HPh2Zu4fMhrUz3Gm7wvq9Dd5KGYMHy0PqtnY36FK0r/d7/p8H+34YaouvVuBW8rjIuDPgW+W8nXAqrZtfru0+05qNKN+ljaeQXWGwzeA21ufF3As8HngrvK8opQH8P7Sxm8Ca/vdhjm2cyHwGHBMW9lAf45UoexBYJTqf3G/2M3nRjXPZEN5vKHf7ZpDGzdQzSFo/U3+WVn3J8vv8DeAm4FXtu1nLVUY+C7wPykXOa3DY4Y2dvy7Wed/d6drYyn/CPArU9Yd1M9xpu+L2vxNemVfSZI0sBxakiRJA8sgI0mSBpZBRpIkDSyDjCRJGlgGGUmSNLAMMpIOm4jYUZ5Pi4ifPcT7fvuU1189lPuXVE8GGUn9cBrQUZBpXUxsFpOCTGb+UId1kjSADDKS+uE9wIsj4paI+E8R0YyIP4yIG8sNBX8ZICJeEhFfiIiPU11ci4j423Ij0NtbNwONiPcAC8r+PlbKWr0/UfZ9W0R8MyJ+pm3fX4yIv46Ib0fEx8pVTCUNkKF+V0DSUely4K2Z+QqAEkieyMwfjIh5wL9ExOfKuucAz87Me8rrX8jMLRGxALgxIj6ZmZdHxJsy8+xpjvUTVDcp/AHguLLNl8t7zwWeRXXPl38BXgR85dA3V1Kv2CMjqQ7Oo7o/yy3ADVSXP19T3vt6W4gB+LWI+AbwNaqb0K1hdj8MfCKrmxU+DHwJ+MG2fW/M6iaGt1ANeUkaIPbISKqDAP5jZk66iVxEvATYOeX1y4EXZuauiPgiMH8O+57J3rblcfw3URo49shI6oftwJK219cBvxoRwwAR8fRy1/KpjgG2lhBzJnBu23ujre2n+DLwM2UezkrgRxicu35LOgj/9yGpH24FxsoQ0UeA/0E1rHNzmXC7GXjVNNv9A/ArEXEr1V2Sv9b23hXArRFxc2b+XFv5p4AXUt11OIH/nJkPlSAkacB592tJkjSwHFqSJEkDyyAjSZIGlkFGkiQNLIOMJEkaWAYZSZI0sAwykiRpYBlkJEnSwDLISJKkgfV/AJ7oC3GvHt0HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. iteration\", size=12)\n",
    "plt.xlabel(\"Iteration\", size=10)\n",
    "plt.ylabel(\"Loss value\", size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring and Calibrating Accuracy with Different Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_test, weight, b):\n",
    "    # Calculates accuracy of predictions\n",
    "    # Input:\n",
    "    # x_test: matrix with features\n",
    "    # weight: vector with final weights\n",
    "    # b: final bias\n",
    "    # Output:\n",
    "    # accuracy: measured from 0 to 1 and multiplied by 100 to get a percentage\n",
    "    scores = y_pred(x_test, weight, b)\n",
    "    pred_labels = (scores > 0.5).astype(int)\n",
    "    pred_labels[pred_labels != 1] = -1\n",
    "    accuracy = np.mean(pred_labels==y_test)\n",
    "    \n",
    "    actual_no_pred_no=0\n",
    "    actual_yes_pred_yes=0\n",
    "    actual_no_pred_yes=0\n",
    "    actual_yes_pred_no=0\n",
    "    \n",
    "    for i in range(scores.shape[0]):\n",
    "        if y_test[i]==pred_labels[i]:\n",
    "            if y_test[i]==-1:\n",
    "                actual_no_pred_no+=1\n",
    "            else:\n",
    "                actual_yes_pred_yes+=1\n",
    "        else:\n",
    "            if y_test[i]==-1:\n",
    "                actual_no_pred_yes+=1\n",
    "            else:\n",
    "                actual_yes_pred_no+=1\n",
    "    print('\\nCONFUSION MATRIX\\n')\n",
    "    print('\\t PREDICTED')\n",
    "    print('ACTUAL\\t NO\\tYES')\n",
    "    print('------------------------------')\n",
    "    print('NO SPAM\\t',actual_no_pred_no,'\\t',actual_no_pred_yes)\n",
    "    print('SPAM\\t',actual_yes_pred_no,'\\t',actual_yes_pred_yes)\n",
    "    print('\\n')\n",
    "    print(\"Error to minimize (Not spam classified as spam):\",round((100*actual_no_pred_yes)/(actual_no_pred_no+actual_yes_pred_yes+actual_no_pred_yes+actual_yes_pred_no),2),'%')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1461 \t 5\n",
      "SPAM\t 20 \t 211\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 0.29 %\n",
      "Features:  475 \t\tAccuracy (%):  98.53\n"
     ]
    }
   ],
   "source": [
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(x_test, weight, b),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.696\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1439 \t 8\n",
      "SPAM\t 17 \t 228\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 0.47 %\n",
      "Features:  184 \t\tAccuracy (%):  98.52\n"
     ]
    }
   ],
   "source": [
    "#Run with less features (top 2%), 2000 iterations and alpha of 0.0008\n",
    "features=feature_decision(spam_text,no_spam_text,0.02)\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "weight, b, losses = logistic_regression(x_train, y_train, 2000, 0.0008)\n",
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(x_test, weight, b),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.704\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1413 \t 13\n",
      "SPAM\t 28 \t 195\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 0.79 %\n",
      "Features:  91 \t\tAccuracy (%):  97.51\n"
     ]
    }
   ],
   "source": [
    "#Run with less features (top 1%), 2000 iterations and alpha of 0.0008\n",
    "features=feature_decision(spam_text,no_spam_text,0.01)\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "weight, b, losses = logistic_regression(x_train, y_train, 2000, 0.0008)\n",
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(x_test, weight, b),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used for training: 0.7\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "\t PREDICTED\n",
      "ACTUAL\t NO\tYES\n",
      "------------------------------\n",
      "NO SPAM\t 1459 \t 4\n",
      "SPAM\t 22 \t 186\n",
      "\n",
      "\n",
      "Error to minimize (Not spam classified as spam): 0.24 %\n",
      "Features:  928 \t\tAccuracy (%):  98.44\n"
     ]
    }
   ],
   "source": [
    "#Run with around twice as many features as the first run (top 10%), 2000 iterations and alpha of 0.0008\n",
    "features=feature_decision(spam_text,no_spam_text,0.1)\n",
    "x_train, y_train, x_test, y_test=create_X_datasets(y_spam, sms_spam, y_no_spam, sms_no_spam, features)\n",
    "weight, b, losses = logistic_regression(x_train, y_train, 2000, 0.0008)\n",
    "print('Features: ', len(features), '\\t\\tAccuracy (%): ',round(100*calculate_accuracy(x_test, weight, b),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The key for a classifier (any, not only binary) is selecting the right features that will allow efficient and accurate results.\n",
    "In this case, adding more features improves the result of the error we prefer not to have (being not spam and classifying it as spam) from 0.29% to 0.24% but the amount of time it takes by doubling the features increases exponentially.\n",
    "If this is a process that is going to be running several times a week with thousands or millions of emails or texts it is better to keep features down to improve running time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
